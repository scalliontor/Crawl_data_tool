#!/usr/bin/env python3
"""
Auto-generate test/groundtruth Q&A pairs from parsed Vietnamese legal documents.

Reads all JSON files in outputs/thue_phi_le_phi/, extracts articles with
substantive content, and generates grounded questions with expected answers.

Question types:
  1. factual          ‚Äî asks about specific provisions/content of an article
  2. case-study       ‚Äî creates a scenario and asks how the law applies
  3. reasoning        ‚Äî asks to explain relationships between articles
  4. hallucination-trap ‚Äî asks about things NOT in the document

Target: 300-400 questions from ~70 diverse documents (stratified by loai_van_ban)

Usage:
    python3 generate_groundtruth.py                  # generate ~350 questions (default)
    python3 generate_groundtruth.py --target 400     # aim for 400 questions
    python3 generate_groundtruth.py --all            # ALL docs (50k+ questions)
    python3 generate_groundtruth.py --stats           # just show stats (dry run)
"""

import json
import glob
import random
import re
import sys
import logging
from pathlib import Path
from collections import Counter, defaultdict
from typing import Optional

logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s", datefmt="%H:%M:%S")
logger = logging.getLogger(__name__)

ROOT = Path(__file__).resolve().parent
PARSED_DIR = ROOT / "outputs" / "thue_phi_le_phi"
OUTPUT_FILE = ROOT / "test_groundtruth.json"

random.seed(42)  # reproducible

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 1. EXTRACT articles/clauses from parsed structure
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def extract_articles(node: dict, doc_title: str = "", depth: int = 0) -> list[dict]:
    """Recursively extract articles and substantive clauses from parsed structure."""
    results = []
    ntype = node.get("type", "")
    title = (node.get("title") or "").strip()
    content = (node.get("content") or "").strip()
    children = node.get("children", [])

    # Collect child content for articles that have children but no/short own content
    children_text = ""
    child_items = []
    for ch in children:
        ch_title = (ch.get("title") or "").strip()
        ch_content = (ch.get("content") or "").strip()
        ch_type = ch.get("type", "")
        piece = f"{ch_title}: {ch_content}" if ch_content else ch_title
        if piece and len(piece) > 10:
            child_items.append({"type": ch_type, "title": ch_title, "content": ch_content})
        if ch_content:
            children_text += " " + ch_content

    full_content = (content + " " + children_text).strip()

    if ntype == "article" and len(full_content) > 60:
        results.append({
            "type": "article",
            "title": title,
            "content": content,
            "full_content": full_content[:3000],  # cap for sanity
            "children": child_items[:20],
            "num_children": len(child_items),
        })

    # Also extract standalone substantive clauses (only if not under a collected article)
    if ntype == "clause" and content and len(content) > 80 and depth >= 2:
        results.append({
            "type": "clause",
            "title": title,
            "content": content[:2000],
            "full_content": content[:2000],
            "children": [],
            "num_children": 0,
        })

    for ch in children:
        results.extend(extract_articles(ch, doc_title, depth + 1))

    return results


def load_document(filepath: str) -> Optional[dict]:
    """Load a parsed JSON document and extract key info."""
    try:
        with open(filepath, encoding="utf-8") as f:
            data = json.load(f)
    except (json.JSONDecodeError, OSError):
        return None

    di = data.get("document_info", {})
    struct = data.get("parsed_result", {}).get("structure", {})
    if not struct:
        return None

    articles = extract_articles(struct, di.get("title", ""))
    # Only keep articles (not bare clauses for most question types)
    article_nodes = [a for a in articles if a["type"] == "article"]

    if not article_nodes:
        return None

    return {
        "filepath": filepath,
        "title": di.get("title", ""),
        "so_hieu": di.get("so_hieu", ""),
        "loai_van_ban": di.get("loai_van_ban", ""),
        "ngay_ban_hanh": di.get("ngay_ban_hanh", ""),
        "noi_ban_hanh": di.get("noi_ban_hanh", ""),
        "tinh_trang": di.get("tinh_trang", "ƒê√£ bi·∫øt"),
        "link": di.get("link", ""),
        "articles": article_nodes,
    }


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 2. QUESTION GENERATION TEMPLATES
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def _clean(text: str, max_len: int = 500) -> str:
    """Clean content for use in answers: dedup lines, trim."""
    # Remove duplicate consecutive lines (parser artifact)
    lines = text.split("\n")
    cleaned = []
    for line in lines:
        line = line.strip()
        if line and (not cleaned or line != cleaned[-1]):
            cleaned.append(line)
    result = "\n".join(cleaned)
    if len(result) > max_len:
        result = result[:max_len] + "..."
    return result


def _extract_dieu_number(title: str) -> str:
    """Extract article number from title like 'ƒêi·ªÅu 7. Thu·∫ø su·∫•t'."""
    m = re.match(r"ƒêi·ªÅu\s+(\d+[\w]*)", title)
    return m.group(0) if m else title[:40]


def _has_list_content(article: dict) -> bool:
    """Check if article has enumerated items (kho·∫£n, ƒëi·ªÉm)."""
    return article["num_children"] >= 2


def _has_numbers(content: str) -> bool:
    """Check if content has specific numbers (percentages, amounts, dates)."""
    return bool(re.search(r'\d+[%,.]?\d*\s*(%|tri·ªáu|ƒë·ªìng|ng√†y|th√°ng|nƒÉm|m¬≤|l·∫ßn)', content))


# ‚îÄ‚îÄ FACTUAL GENERATORS ‚îÄ‚îÄ

def gen_factual_content(doc: dict, article: dict) -> Optional[dict]:
    """Generate a factual question about the content of an article."""
    dieu = _extract_dieu_number(article["title"])
    content = _clean(article["full_content"], 800)

    if len(content) < 60:
        return None

    so_hieu = doc["so_hieu"]
    loai = doc["loai_van_ban"]

    query = f"Theo {dieu} {loai} {so_hieu}, n·ªôi dung quy ƒë·ªãnh c·ª• th·ªÉ l√† g√¨?"

    return {
        "type": "factual",
        "source_doc": so_hieu,
        "source_title": doc["title"],
        "article_ref": article["title"],
        "query": query,
        "expected_answer": f"Theo {dieu} {loai} {so_hieu} ({doc['title']}): {content}",
    }


def gen_factual_list(doc: dict, article: dict) -> Optional[dict]:
    """Generate a factual question about a list/enumeration in an article."""
    if not _has_list_content(article):
        return None

    dieu = _extract_dieu_number(article["title"])
    so_hieu = doc["so_hieu"]
    loai = doc["loai_van_ban"]
    n = article["num_children"]

    child_summary = "; ".join(
        f"({i+1}) {_clean(ch['title'], 100)}" + (f": {_clean(ch['content'], 150)}" if ch['content'] else "")
        for i, ch in enumerate(article["children"][:15])
    )

    query = f"{dieu} {loai} {so_hieu} li·ªát k√™ bao nhi√™u tr∆∞·ªùng h·ª£p/kho·∫£n v√† n·ªôi dung c·ª• th·ªÉ l√† g√¨?"

    return {
        "type": "factual",
        "source_doc": so_hieu,
        "source_title": doc["title"],
        "article_ref": article["title"],
        "query": query,
        "expected_answer": f"{dieu} li·ªát k√™ {n} kho·∫£n/tr∆∞·ªùng h·ª£p: {child_summary}",
    }


def gen_factual_number(doc: dict, article: dict) -> Optional[dict]:
    """Generate a factual question about specific numbers in an article."""
    content = article["full_content"]
    if not _has_numbers(content):
        return None

    dieu = _extract_dieu_number(article["title"])
    so_hieu = doc["so_hieu"]
    loai = doc["loai_van_ban"]

    # Find the specific numbers
    numbers = re.findall(r'(\d+[.,]?\d*\s*(?:%|tri·ªáu|ƒë·ªìng|ng√†y|th√°ng|nƒÉm|m¬≤|l·∫ßn|gi·ªù|ph√∫t))', content)
    if not numbers:
        return None

    query = f"C√°c m·ª©c/con s·ªë c·ª• th·ªÉ ƒë∆∞·ª£c quy ƒë·ªãnh t·∫°i {dieu} {loai} {so_hieu} l√† bao nhi√™u?"

    answer_content = _clean(content, 600)
    return {
        "type": "factual",
        "source_doc": so_hieu,
        "source_title": doc["title"],
        "article_ref": article["title"],
        "query": query,
        "expected_answer": f"Theo {dieu}: {answer_content}",
    }


def gen_factual_scope(doc: dict) -> Optional[dict]:
    """Generate a question about the scope/applicability of the document."""
    # Find ƒêi·ªÅu 1 (ph·∫°m vi ƒëi·ªÅu ch·ªânh) or ƒêi·ªÅu 2 (ƒë·ªëi t∆∞·ª£ng √°p d·ª•ng)
    for art in doc["articles"][:5]:
        title_lower = art["title"].lower()
        if any(k in title_lower for k in ["ph·∫°m vi", "ƒëi·ªÅu ch·ªânh", "ƒë·ªëi t∆∞·ª£ng √°p d·ª•ng", "√°p d·ª•ng"]):
            content = _clean(art["full_content"], 600)
            if len(content) < 40:
                continue
            so_hieu = doc["so_hieu"]
            loai = doc["loai_van_ban"]
            query = f"{loai} {so_hieu} quy ƒë·ªãnh v·ªÅ v·∫•n ƒë·ªÅ g√¨ v√† √°p d·ª•ng cho ƒë·ªëi t∆∞·ª£ng n√†o?"
            return {
                "type": "factual",
                "source_doc": so_hieu,
                "source_title": doc["title"],
                "article_ref": art["title"],
                "query": query,
                "expected_answer": f"Theo {art['title']} {loai} {so_hieu} ({doc['title']}): {content}",
            }
    return None


def gen_factual_effective_date(doc: dict) -> Optional[dict]:
    """Generate a question about effective date / validity."""
    # Find the last article (usually about hi·ªáu l·ª±c)
    for art in reversed(doc["articles"]):
        title_lower = art["title"].lower()
        if any(k in title_lower for k in ["hi·ªáu l·ª±c", "thi h√†nh", "ƒëi·ªÅu kho·∫£n"]):
            content = _clean(art["full_content"], 400)
            if len(content) < 30:
                continue
            so_hieu = doc["so_hieu"]
            loai = doc["loai_van_ban"]

            # Build expected answer including tinh_trang
            ts = doc["tinh_trang"]
            answer = f"Theo {art['title']}: {content}"
            if ts and ts != "ƒê√£ bi·∫øt":
                answer += f" T√¨nh tr·∫°ng hi·ªán t·∫°i: {ts}."

            return {
                "type": "factual",
                "source_doc": so_hieu,
                "source_title": doc["title"],
                "article_ref": art["title"],
                "query": f"{loai} {so_hieu} c√≥ hi·ªáu l·ª±c t·ª´ khi n√†o v√† t√¨nh tr·∫°ng hi·ªáu l·ª±c hi·ªán t·∫°i?",
                "expected_answer": answer,
            }
    return None


# ‚îÄ‚îÄ CASE-STUDY GENERATORS ‚îÄ‚îÄ

CASE_TEMPLATES = [
    {
        "condition": lambda doc, art: "thu·∫ø" in art["full_content"].lower() and _has_numbers(art["full_content"]),
        "gen_case": lambda doc, art: f"M·ªôt doanh nghi·ªáp/c√° nh√¢n c·∫ßn √°p d·ª•ng quy ƒë·ªãnh t·∫°i {_extract_dieu_number(art['title'])} {doc['loai_van_ban']} {doc['so_hieu']}.",
        "gen_query": lambda doc, art: f"Theo {_extract_dieu_number(art['title'])}, quy ƒë·ªãnh n√†y √°p d·ª•ng c·ª• th·ªÉ nh∆∞ th·∫ø n√†o cho tr∆∞·ªùng h·ª£p n√™u tr√™n?",
    },
    {
        "condition": lambda doc, art: any(k in art["full_content"].lower() for k in ["x·ª≠ ph·∫°t", "vi ph·∫°m", "ph·∫°t ti·ªÅn", "c∆∞·ª°ng ch·∫ø"]),
        "gen_case": lambda doc, art: f"M·ªôt t·ªï ch·ª©c/c√° nh√¢n vi ph·∫°m quy ƒë·ªãnh t·∫°i {_extract_dieu_number(art['title'])} {doc['loai_van_ban']} {doc['so_hieu']}.",
        "gen_query": lambda doc, art: f"H√¨nh th·ª©c x·ª≠ l√Ω v√† m·ª©c ph·∫°t c·ª• th·ªÉ theo {_extract_dieu_number(art['title'])} l√† g√¨?",
    },
    {
        "condition": lambda doc, art: any(k in art["full_content"].lower() for k in ["mi·ªÖn", "gi·∫£m", "∆∞u ƒë√£i", "kh√¥ng ch·ªãu thu·∫ø", "kh√¥ng ph·∫£i n·ªôp"]),
        "gen_case": lambda doc, art: f"M·ªôt ƒë·ªëi t∆∞·ª£ng mu·ªën bi·∫øt m√¨nh c√≥ thu·ªôc di·ªán mi·ªÖn/gi·∫£m theo {_extract_dieu_number(art['title'])} {doc['loai_van_ban']} {doc['so_hieu']} kh√¥ng.",
        "gen_query": lambda doc, art: f"Theo {_extract_dieu_number(art['title'])}, nh·ªØng tr∆∞·ªùng h·ª£p n√†o ƒë∆∞·ª£c mi·ªÖn/gi·∫£m v√† ƒëi·ªÅu ki·ªán c·ª• th·ªÉ l√† g√¨?",
    },
    {
        "condition": lambda doc, art: any(k in art["full_content"].lower() for k in ["th·ªß t·ª•c", "h·ªì s∆°", "tr√¨nh t·ª±", "ƒëƒÉng k√Ω", "k√™ khai"]),
        "gen_case": lambda doc, art: f"M·ªôt ng∆∞·ªùi n·ªôp thu·∫ø c·∫ßn th·ª±c hi·ªán th·ªß t·ª•c theo {_extract_dieu_number(art['title'])} {doc['loai_van_ban']} {doc['so_hieu']}.",
        "gen_query": lambda doc, art: f"Tr√¨nh t·ª±, th·ªß t·ª•c v√† h·ªì s∆° c·∫ßn thi·∫øt theo {_extract_dieu_number(art['title'])} bao g·ªìm nh·ªØng g√¨?",
    },
    {
        "condition": lambda doc, art: any(k in art["full_content"].lower() for k in ["tr√°ch nhi·ªám", "nghƒ©a v·ª•", "quy·ªÅn", "quy·ªÅn h·∫°n"]),
        "gen_case": lambda doc, art: f"C·∫ßn x√°c ƒë·ªãnh tr√°ch nhi·ªám/quy·ªÅn h·∫°n c·ªßa c√°c b√™n theo {_extract_dieu_number(art['title'])} {doc['loai_van_ban']} {doc['so_hieu']}.",
        "gen_query": lambda doc, art: f"Theo {_extract_dieu_number(art['title'])}, tr√°ch nhi·ªám v√† quy·ªÅn h·∫°n c·ª• th·ªÉ c·ªßa c√°c b√™n ƒë∆∞·ª£c quy ƒë·ªãnh nh∆∞ th·∫ø n√†o?",
    },
]


def gen_case_study(doc: dict, article: dict) -> Optional[dict]:
    """Generate a case-study question from matching templates."""
    for tmpl in CASE_TEMPLATES:
        try:
            if tmpl["condition"](doc, article):
                case = tmpl["gen_case"](doc, article)
                query = tmpl["gen_query"](doc, article)
                content = _clean(article["full_content"], 800)

                return {
                    "type": "case-study",
                    "source_doc": doc["so_hieu"],
                    "source_title": doc["title"],
                    "article_ref": article["title"],
                    "case": case,
                    "query": query,
                    "expected_answer": f"Theo {_extract_dieu_number(article['title'])} {doc['loai_van_ban']} {doc['so_hieu']}: {content}",
                }
        except Exception:
            continue
    return None


# ‚îÄ‚îÄ REASONING GENERATORS ‚îÄ‚îÄ

def gen_reasoning_multi_article(doc: dict) -> Optional[dict]:
    """Generate a reasoning question linking multiple articles in one doc."""
    if len(doc["articles"]) < 3:
        return None

    # Pick 2-3 related articles
    arts = random.sample(doc["articles"][:min(10, len(doc["articles"]))], min(3, len(doc["articles"])))
    dieus = [_extract_dieu_number(a["title"]) for a in arts]
    so_hieu = doc["so_hieu"]
    loai = doc["loai_van_ban"]

    query = f"Gi·∫£i th√≠ch m·ªëi quan h·ªá v√† logic gi·ªØa {', '.join(dieus)} trong {loai} {so_hieu}."

    parts = []
    for a in arts:
        parts.append(f"- {_extract_dieu_number(a['title'])}: {_clean(a['full_content'], 250)}")

    return {
        "type": "reasoning",
        "source_doc": so_hieu,
        "source_title": doc["title"],
        "article_ref": [a["title"] for a in arts],
        "query": query,
        "expected_answer": f"Trong {loai} {so_hieu} ({doc['title']}), c√°c ƒëi·ªÅu kho·∫£n li√™n h·ªá nh∆∞ sau:\n" + "\n".join(parts),
    }


def gen_reasoning_compare_status(doc: dict) -> Optional[dict]:
    """Generate a reasoning question about document status and implications."""
    ts = doc["tinh_trang"]
    if ts in ("ƒê√£ bi·∫øt", "", None):
        return None

    so_hieu = doc["so_hieu"]
    loai = doc["loai_van_ban"]

    query = f"{loai} {so_hieu} hi·ªán c√≥ t√¨nh tr·∫°ng '{ts}'. ƒêi·ªÅu n√†y c√≥ √Ω nghƒ©a g√¨ v·ªÅ m·∫∑t ph√°p l√Ω khi √°p d·ª•ng vƒÉn b·∫£n?"

    # Find effective date article
    eff_content = ""
    for art in reversed(doc["articles"]):
        if any(k in art["title"].lower() for k in ["hi·ªáu l·ª±c", "thi h√†nh"]):
            eff_content = _clean(art["full_content"], 300)
            break

    answer = f"{loai} {so_hieu} ({doc['title']}) c√≥ t√¨nh tr·∫°ng: {ts}."
    if ts == "H·∫øt hi·ªáu l·ª±c":
        answer += " VƒÉn b·∫£n ƒë√£ h·∫øt hi·ªáu l·ª±c ph√°p lu·∫≠t, kh√¥ng c√≤n ƒë∆∞·ª£c √°p d·ª•ng. C√°c quy ƒë·ªãnh trong vƒÉn b·∫£n ƒë√£ ƒë∆∞·ª£c thay th·∫ø b·ªüi vƒÉn b·∫£n m·ªõi."
    elif ts == "C√≤n hi·ªáu l·ª±c":
        answer += " VƒÉn b·∫£n ƒëang c√≥ hi·ªáu l·ª±c ph√°p lu·∫≠t, c√°c quy ƒë·ªãnh trong vƒÉn b·∫£n v·∫´n ƒë∆∞·ª£c √°p d·ª•ng."
    elif ts == "H·∫øt hi·ªáu l·ª±c m·ªôt ph·∫ßn":
        answer += " M·ªôt s·ªë ƒëi·ªÅu/kho·∫£n trong vƒÉn b·∫£n ƒë√£ b·ªã s·ª≠a ƒë·ªïi, b·ªï sung ho·∫∑c b√£i b·ªè b·ªüi vƒÉn b·∫£n kh√°c, nh∆∞ng ph·∫ßn c√≤n l·∫°i v·∫´n c√≥ hi·ªáu l·ª±c."
    elif ts == "T·∫°m ng∆∞ng hi·ªáu l·ª±c":
        answer += " VƒÉn b·∫£n t·∫°m th·ªùi kh√¥ng ƒë∆∞·ª£c √°p d·ª•ng, ch·ªù quy·∫øt ƒë·ªãnh t·ª´ c∆° quan c√≥ th·∫©m quy·ªÅn."
    if eff_content:
        answer += f" {eff_content}"

    return {
        "type": "reasoning",
        "source_doc": so_hieu,
        "source_title": doc["title"],
        "article_ref": "document_info.tinh_trang",
        "query": query,
        "expected_answer": answer,
    }


# ‚îÄ‚îÄ HALLUCINATION-TRAP GENERATORS ‚îÄ‚îÄ

TRAP_TEMPLATES = [
    {
        "gen": lambda doc: {
            "query": f"Theo {doc['loai_van_ban']} {doc['so_hieu']}, m·ª©c ph·∫°t t√π t·ªëi ƒëa cho vi ph·∫°m quy ƒë·ªãnh n√†y l√† bao nhi√™u nƒÉm?",
            "expected_answer": f"{doc['loai_van_ban']} {doc['so_hieu']} ({doc['title']}) KH√îNG quy ƒë·ªãnh v·ªÅ h√¨nh ph·∫°t t√π. VƒÉn b·∫£n n√†y ch·ªâ quy ƒë·ªãnh v·ªÅ x·ª≠ l√Ω h√†nh ch√≠nh/n·ªôi dung qu·∫£n l√Ω thu·∫ø-ph√≠-l·ªá ph√≠. H√¨nh ph·∫°t t√π thu·ªôc ph·∫°m vi ƒëi·ªÅu ch·ªânh c·ªßa B·ªô lu·∫≠t H√¨nh s·ª±.",
        },
    },
    {
        "gen": lambda doc: {
            "query": f"{doc['loai_van_ban']} {doc['so_hieu']} c√≥ quy ƒë·ªãnh c·ª• th·ªÉ v·ªÅ thu·∫ø su·∫•t VAT cho h√†ng h√≥a xu·∫•t kh·∫©u qua s√†n th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠ xuy√™n bi√™n gi·ªõi kh√¥ng?",
            "expected_answer": f"{doc['loai_van_ban']} {doc['so_hieu']} ({doc['title']}) KH√îNG c√≥ quy ƒë·ªãnh ri√™ng v·ªÅ thu·∫ø su·∫•t VAT cho th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠ xuy√™n bi√™n gi·ªõi. C·∫ßn tra c·ª©u c√°c vƒÉn b·∫£n chuy√™n bi·ªát v·ªÅ th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠ v√† thu·∫ø GTGT xu·∫•t kh·∫©u.",
        },
    },
    {
        "gen": lambda doc: {
            "query": f"{doc['loai_van_ban']} {doc['so_hieu']} ƒë∆∞·ª£c ban h√†nh b·ªüi Qu·ªëc h·ªôi ng√†y {doc.get('ngay_ban_hanh', '??')} quy ƒë·ªãnh c·ª• th·ªÉ ti√™u chu·∫©n ISO n√†o ph·∫£i tu√¢n th·ªß?",
            "expected_answer": f"{doc['loai_van_ban']} {doc['so_hieu']} ({doc['title']}) KH√îNG ƒë·ªÅ c·∫≠p ƒë·∫øn b·∫•t k·ª≥ ti√™u chu·∫©n ISO c·ª• th·ªÉ n√†o. C√¢u h·ªèi ch·ª©a th√¥ng tin g√¢y hi·ªÉu nh·∫ßm. N∆°i ban h√†nh th·ª±c t·∫ø: {doc.get('noi_ban_hanh', 'kh√¥ng r√µ')} (kh√¥ng nh·∫•t thi·∫øt l√† Qu·ªëc h·ªôi).",
        },
    },
    {
        "gen": lambda doc: {
            "query": f"Li·ªát k√™ c√°c h√¨nh th·ª©c x·ª≠ ph·∫°t h√¨nh s·ª± m√† {doc['loai_van_ban']} {doc['so_hieu']} quy ƒë·ªãnh cho t·ªôi tham nh≈©ng trong lƒ©nh v·ª±c thu·∫ø.",
            "expected_answer": f"{doc['loai_van_ban']} {doc['so_hieu']} ({doc['title']}) KH√îNG quy ƒë·ªãnh v·ªÅ x·ª≠ ph·∫°t h√¨nh s·ª± hay t·ªôi tham nh≈©ng. ƒê√¢y l√† vƒÉn b·∫£n thu·ªôc lƒ©nh v·ª±c Thu·∫ø-Ph√≠-L·ªá ph√≠, kh√¥ng ph·∫£i B·ªô lu·∫≠t H√¨nh s·ª±. B·∫•t k·ª≥ c√¢u tr·∫£ l·ªùi n√†o li·ªát k√™ h√¨nh ph·∫°t h√¨nh s·ª± ƒë·ªÅu l√† b·ªãa ƒë·∫∑t.",
        },
    },
    {
        "gen": lambda doc: {
            "query": f"{doc['loai_van_ban']} {doc['so_hieu']} quy ƒë·ªãnh doanh nghi·ªáp ph·∫£i n·ªôp b√°o c√°o ESG (Environment, Social, Governance) cho c∆° quan thu·∫ø nh∆∞ th·∫ø n√†o?",
            "expected_answer": f"{doc['loai_van_ban']} {doc['so_hieu']} ({doc['title']}) KH√îNG c√≥ b·∫•t k·ª≥ quy ƒë·ªãnh n√†o v·ªÅ b√°o c√°o ESG. ƒê√¢y l√† kh√°i ni·ªám thu·ªôc lƒ©nh v·ª±c qu·∫£n tr·ªã doanh nghi·ªáp/ch·ª©ng kho√°n, kh√¥ng li√™n quan ƒë·∫øn n·ªôi dung vƒÉn b·∫£n thu·∫ø-ph√≠-l·ªá ph√≠ n√†y.",
        },
    },
]


def gen_hallucination_trap(doc: dict) -> Optional[dict]:
    """Generate a hallucination-trap question for a document."""
    tmpl = random.choice(TRAP_TEMPLATES)
    try:
        result = tmpl["gen"](doc)
        return {
            "type": "hallucination-trap",
            "source_doc": doc["so_hieu"],
            "source_title": doc["title"],
            "query": result["query"],
            "expected_answer": result["expected_answer"],
        }
    except Exception:
        return None


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 3. MAIN GENERATION PIPELINE
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def generate_questions_for_doc(doc: dict, max_per_doc: int = 4) -> list[dict]:
    """Generate a balanced mix of question types for a single document.

    Produces up to max_per_doc questions. Default 4 = 2 factual + 1 case/reasoning
    + 1 hallucination-trap.
    """
    articles = doc["articles"]
    if not articles:
        return []

    candidates: dict[str, list[dict]] = {
        "factual": [],
        "case-study": [],
        "reasoning": [],
        "hallucination-trap": [],
    }

    substantial = [a for a in articles if len(a["full_content"]) > 100]

    # ‚îÄ‚îÄ Collect FACTUAL candidates ‚îÄ‚îÄ
    q = gen_factual_scope(doc)
    if q:
        candidates["factual"].append(q)

    if substantial:
        art = random.choice(substantial)
        q = gen_factual_content(doc, art)
        if q:
            candidates["factual"].append(q)

    list_articles = [a for a in articles if _has_list_content(a)]
    if list_articles:
        art = random.choice(list_articles)
        q = gen_factual_list(doc, art)
        if q:
            candidates["factual"].append(q)

    number_articles = [a for a in articles if _has_numbers(a["full_content"])]
    if number_articles:
        art = random.choice(number_articles)
        q = gen_factual_number(doc, art)
        if q:
            candidates["factual"].append(q)

    q = gen_factual_effective_date(doc)
    if q:
        candidates["factual"].append(q)

    # ‚îÄ‚îÄ Collect CASE-STUDY candidates ‚îÄ‚îÄ
    if substantial:
        art = random.choice(substantial)
        q = gen_case_study(doc, art)
        if q:
            candidates["case-study"].append(q)

    # ‚îÄ‚îÄ Collect REASONING candidates ‚îÄ‚îÄ
    if len(articles) >= 3:
        q = gen_reasoning_multi_article(doc)
        if q:
            candidates["reasoning"].append(q)
    q = gen_reasoning_compare_status(doc)
    if q:
        candidates["reasoning"].append(q)

    # ‚îÄ‚îÄ Collect HALLUCINATION-TRAP candidates ‚îÄ‚îÄ
    q = gen_hallucination_trap(doc)
    if q:
        candidates["hallucination-trap"].append(q)

    # ‚îÄ‚îÄ Balanced selection up to max_per_doc ‚îÄ‚îÄ
    # Target mix: 2 factual, 1 case-study OR reasoning, 1 hallucination-trap
    selected = []

    # 1. Pick up to 2 factual
    facts = candidates["factual"]
    random.shuffle(facts)
    selected.extend(facts[:2])

    # 2. Pick 1 case-study (prefer) or reasoning
    if candidates["case-study"]:
        selected.append(candidates["case-study"][0])
    elif candidates["reasoning"]:
        selected.append(candidates["reasoning"][0])

    # 3. Pick 1 hallucination-trap
    if candidates["hallucination-trap"]:
        selected.append(candidates["hallucination-trap"][0])

    # 4. If room, add 1 more (reasoning > factual > case-study)
    if len(selected) < max_per_doc:
        for pool_name in ["reasoning", "factual", "case-study"]:
            for q in candidates[pool_name]:
                if q not in selected:
                    selected.append(q)
                    break
            if len(selected) >= max_per_doc:
                break

    return selected[:max_per_doc]


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 4. STRATIFIED SAMPLING ‚Äî pick diverse, high-quality docs
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

# Priority loai_van_ban (most legally substantive)
PRIORITY_TYPES = [
    "Lu·∫≠t",
    "Ngh·ªã ƒë·ªãnh",
    "Th√¥ng t∆∞",
    "Th√¥ng t∆∞ li√™n t·ªãch",
    "VƒÉn b·∫£n h·ª£p nh·∫•t",
    "Ngh·ªã quy·∫øt",
    "Quy·∫øt ƒë·ªãnh",
    "Ch·ªâ th·ªã",
]


def _doc_quality_score(doc: dict) -> float:
    """Score a doc for selection: more articles, enrichment, content = higher."""
    score = 0.0
    # Enriched docs are much more valuable
    if doc["tinh_trang"] not in ("ƒê√£ bi·∫øt", "", None):
        score += 50
    # More articles = richer doc
    score += min(len(doc["articles"]), 20) * 3
    # Has list content
    if any(_has_list_content(a) for a in doc["articles"]):
        score += 10
    # Has numbers
    if any(_has_numbers(a["full_content"]) for a in doc["articles"]):
        score += 10
    # Total content length (proxy for substantiveness)
    total_len = sum(len(a["full_content"]) for a in doc["articles"])
    score += min(total_len / 500, 20)
    return score


def sample_documents(docs: list[dict], target_questions: int) -> list[dict]:
    """Stratified sampling: pick diverse docs across loai_van_ban to hit target.

    Avg ~5 questions/doc, so need target/5 docs.
    Allocates slots proportionally to loai_van_ban, with minimum
    representation for priority types.
    """
    n_docs_needed = max(target_questions // 5, 40)  # ~5 Q/doc
    logger.info("Need ~%d docs for ~%d questions", n_docs_needed, target_questions)

    # Group by loai_van_ban
    by_type: dict[str, list[dict]] = defaultdict(list)
    for d in docs:
        by_type[d["loai_van_ban"] or "Kh√°c"].append(d)

    # Sort each group by quality (best first)
    for grp in by_type.values():
        grp.sort(key=_doc_quality_score, reverse=True)

    # Allocate slots: priority types get guaranteed minimums
    allocation: dict[str, int] = {}
    remaining = n_docs_needed

    # Phase 1: Guarantee at least some from each priority type that exists
    for lt in PRIORITY_TYPES:
        if lt in by_type and by_type[lt]:
            count = min(max(2, len(by_type[lt]) * n_docs_needed // len(docs)), len(by_type[lt]))
            # Boost for Lu·∫≠t / Ngh·ªã ƒë·ªãnh / Th√¥ng t∆∞ (most important)
            if lt in ("Lu·∫≠t", "Ngh·ªã ƒë·ªãnh", "Th√¥ng t∆∞"):
                count = min(count + 5, len(by_type[lt]))
            allocation[lt] = count
            remaining -= count

    # Phase 2: Fill remaining with best-scoring docs from non-allocated types
    other_types = [lt for lt in by_type if lt not in allocation]
    for lt in other_types:
        if remaining <= 0:
            break
        count = min(max(1, remaining // max(len(other_types), 1)), len(by_type[lt]))
        allocation[lt] = count
        remaining -= count

    # Phase 3: If still have room, add more from top-scoring types
    if remaining > 0:
        for lt in PRIORITY_TYPES:
            if remaining <= 0:
                break
            if lt in by_type:
                can_add = len(by_type[lt]) - allocation.get(lt, 0)
                add = min(remaining, can_add)
                allocation[lt] = allocation.get(lt, 0) + add
                remaining -= add

    # Select docs
    selected = []
    for lt, count in allocation.items():
        grp = by_type[lt]
        # Pick top-quality docs, but also sprinkle a few random ones for diversity
        top_n = min(count, len(grp))
        if top_n <= 3:
            selected.extend(grp[:top_n])
        else:
            # Top 60% by quality + 40% random from the rest
            n_top = max(top_n * 3 // 5, 1)
            n_rand = top_n - n_top
            selected.extend(grp[:n_top])
            rest = grp[n_top:]
            if rest and n_rand > 0:
                selected.extend(random.sample(rest, min(n_rand, len(rest))))

    random.shuffle(selected)

    logger.info("Selected %d docs across %d loai_van_ban types:", len(selected), len(allocation))
    sel_types = Counter(d["loai_van_ban"] or "Kh√°c" for d in selected)
    for lt, c in sel_types.most_common():
        logger.info("  %-25s: %d", lt, c)

    return selected


def main():
    target = 350
    stats_only = False
    all_docs_mode = False

    args = sys.argv[1:]
    i = 0
    while i < len(args):
        if args[i] == "--stats":
            stats_only = True
        elif args[i] == "--all":
            all_docs_mode = True
        elif args[i] == "--target" and i + 1 < len(args):
            target = int(args[i + 1])
            i += 1
        i += 1

    # Load all documents
    files = sorted(glob.glob(str(PARSED_DIR / "**" / "*.json"), recursive=True))
    logger.info("Found %d JSON files", len(files))

    docs = []
    for i, fp in enumerate(files):
        doc = load_document(fp)
        if doc:
            docs.append(doc)
        if (i + 1) % 2000 == 0:
            logger.info("  Loaded %d/%d files (%d valid docs)...", i + 1, len(files), len(docs))

    logger.info("Loaded %d valid documents (with articles)", len(docs))

    # Unless --all, sample a diverse subset to hit the target
    if not all_docs_mode:
        docs = sample_documents(docs, target)

    if stats_only:
        _show_stats(docs)
        return

    # Generate questions
    all_questions = []
    type_counts = Counter()
    docs_with_q = 0

    for doc in docs:
        qs = generate_questions_for_doc(doc)
        if qs:
            docs_with_q += 1
        for q in qs:
            type_counts[q["type"]] += 1
        all_questions.extend(qs)

    # Shuffle for variety
    random.shuffle(all_questions)

    # Save
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(all_questions, f, ensure_ascii=False, indent=2)

    # Summary
    print()
    print("=" * 60)
    print("‚úÖ TEST GROUNDTRUTH GENERATION COMPLETE")
    print("=" * 60)
    print(f"  Documents sampled:     {len(docs)}")
    print(f"  Documents with Q&A:    {docs_with_q}")
    print(f"  Total questions:       {len(all_questions)}")
    print(f"  Avg per doc:           {len(all_questions)/max(docs_with_q,1):.1f}")
    print()
    print("  By type:")
    for t, c in type_counts.most_common():
        print(f"    {t:25s}: {c:5d} ({c/len(all_questions)*100:.1f}%)")
    print()
    print("  By loai_van_ban:")
    lvb = Counter(q["source_doc"] for q in all_questions)
    lvb_type = Counter()
    doc_map = {d["so_hieu"]: d["loai_van_ban"] for d in docs}
    for sh, cnt in lvb.items():
        lvb_type[doc_map.get(sh, "?")] += cnt
    for lt, c in lvb_type.most_common():
        print(f"    {lt:25s}: {c:5d}")
    print()
    print(f"  Output: {OUTPUT_FILE}")
    print("=" * 60)


def _show_stats(docs):
    """Show statistics about what would be generated."""
    type_counts = Counter()
    docs_with_q = 0

    for doc in docs:
        qs = generate_questions_for_doc(doc)
        if qs:
            docs_with_q += 1
        for q in qs:
            type_counts[q["type"]] += 1

    total = sum(type_counts.values())
    print()
    print("=" * 60)
    print("üìä GENERATION STATISTICS (dry run)")
    print("=" * 60)
    print(f"  Documents:           {len(docs)}")
    print(f"  Docs with questions: {docs_with_q}")
    print(f"  Total questions:     {total}")
    print(f"  Avg per doc:         {total/max(docs_with_q,1):.1f}")
    print()
    for t, c in type_counts.most_common():
        print(f"    {t:25s}: {c:6d} ({c/total*100:.1f}%)")
    print("=" * 60)


if __name__ == "__main__":
    main()
